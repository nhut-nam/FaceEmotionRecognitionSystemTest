{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4fb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44925239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\ProjectAI\\\\FaceEmotionRecognitionSystem'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f8b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path \n",
    "    updated_base_model_path: Path\n",
    "    training_data_path: Path\n",
    "    dataset_path: Path\n",
    "    model_name: str\n",
    "    optimizer_name: str\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_learning_rate: float\n",
    "    params_num_classes: int\n",
    "    params_image_size: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242e00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FacialExpressionRecognition.constants import *\n",
    "from FacialExpressionRecognition.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f5e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training_config = self.config.training\n",
    "        artifacts_root = self.config.artifacts_root\n",
    "        trained_model_path = os.path.join(training_config.trained_model_path)\n",
    "        updated_base_model_path = os.path.join(self.config.prepare_base_model.updated_base_model_path)\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"data.zip\")\n",
    "        dataset_path = os.path.join(training_config.dataset_path)\n",
    "\n",
    "        create_directories([Path(training_config.root_dir)])\n",
    "\n",
    "        params = self.params\n",
    "        params_epochs = params.EPOCHS\n",
    "        params_batch_size = params.BATCH_SIZE\n",
    "        params_is_augmentation = params.AUGMENTATION\n",
    "        params_learning_rate = params.LEARNING_RATE\n",
    "        params_num_classes = params.NUM_CLASSES\n",
    "        params_image_size = params.IMAGE_SIZE\n",
    "        params_model_name = params.MODEL_NAME\n",
    "        params_optimizer_name = params.OPTIMIZER\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(artifacts_root),\n",
    "            trained_model_path=Path(trained_model_path),\n",
    "            updated_base_model_path=Path(updated_base_model_path),\n",
    "            training_data_path=Path(training_data),\n",
    "            dataset_path=Path(dataset_path),\n",
    "            model_name=params_model_name,\n",
    "            optimizer_name=params_optimizer_name,\n",
    "            params_epochs=params_epochs,\n",
    "            params_batch_size=params_batch_size,\n",
    "            params_is_augmentation=params_is_augmentation,\n",
    "            params_learning_rate=params_learning_rate,\n",
    "            params_num_classes=params_num_classes,\n",
    "            params_image_size=params_image_size\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ed0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from FacialExpressionRecognition import logger\n",
    "from FacialExpressionRecognition.entity.config_entity import TrainingConfig\n",
    "from FacialExpressionRecognition.models.resnet34 import get_resnet34_model\n",
    "from FacialExpressionRecognition.models.emonext import get_model \n",
    "import numpy as np\n",
    "from torch.cuda.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e825b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.early_stopping_patience = 12\n",
    "        self.best_val_accuracy = 0\n",
    "        self.scaler = GradScaler(enabled=False)\n",
    "    \n",
    "    def get_model(self):\n",
    "        if self.config.model_name == \"resnet34\":\n",
    "            self.model = get_resnet34_model(num_classes=self.config.params_num_classes)\n",
    "            self.model.load_state_dict(torch.load(self.config.updated_base_model_path))\n",
    "            self.model.eval()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif self.config.model_name == \"emonext\":\n",
    "            self.model = get_model(num_classes=self.config.params_num_classes)\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            raise ValueError(f\"Model {self.config.model_name} not supported.\")\n",
    "        \n",
    "\n",
    "    def extract_dataset(self):\n",
    "        with ZipFile(self.config.training_data_path, 'r') as zip_ref:\n",
    "            os.makedirs(self.config.dataset_path, exist_ok=True)\n",
    "            zip_ref.extractall(self.config.dataset_path)\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        if self.config.optimizer_name == \"adam\":\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        elif self.config.optimizer_name == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.params_learning_rate, momentum=0.9)\n",
    "        elif self.config.optimizer_name == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer {self.config.optimizer_name} not supported.\")\n",
    "        return optimizer\n",
    "\n",
    "    def train(self):\n",
    "        train_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.RandomVerticalFlip(),\n",
    "            torchvision.transforms.Grayscale(),\n",
    "            torchvision.transforms.Resize(140),\n",
    "            torchvision.transforms.RandomRotation(degrees=20),\n",
    "            torchvision.transforms.RandomCrop(self.config.params_image_size[0]),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        ])\n",
    "\n",
    "        val_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Grayscale(),\n",
    "            torchvision.transforms.Resize(140),\n",
    "            torchvision.transforms.RandomCrop(self.config.params_image_size[0]),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        ])\n",
    "\n",
    "        test_transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Grayscale(),\n",
    "            torchvision.transforms.Resize(140),\n",
    "            torchvision.transforms.TenCrop(self.config.params_image_size[0]),\n",
    "            torchvision.transforms.Lambda(\n",
    "                lambda crops: torch.stack(\n",
    "                    [torchvision.transforms.ToTensor()(crop) for crop in crops]\n",
    "                )\n",
    "            ),\n",
    "            torchvision.transforms.Lambda(\n",
    "                lambda crops: torch.stack([crop.repeat(3, 1, 1) for crop in crops])\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        self.extract_dataset()\n",
    "        \n",
    "        train_data = torchvision.datasets.ImageFolder(root=self.config.dataset_path / \"FER2013\" / \"train\", transform=train_transforms)\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_data, batch_size=self.config.params_batch_size, shuffle=True)\n",
    "\n",
    "        val_data = torchvision.datasets.ImageFolder(root=self.config.dataset_path / \"FER2013\" / \"val\", transform=val_transforms)\n",
    "        self.val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=False)\n",
    "\n",
    "        test_data = torchvision.datasets.ImageFolder(root=self.config.dataset_path / \"FER2013\" / \"test\", transform=test_transform)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.get_model()\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = self.get_optimizer()\n",
    "\n",
    "        num_epochs = self.config.params_epochs\n",
    "\n",
    "        counter = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            val_loss, val_accuracy = self.val_epoch()\n",
    "            logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "            if val_accuracy > self.best_val_accuracy:\n",
    "                self.save_model(self.model, self.config.trained_model_path)\n",
    "                counter = 0\n",
    "                self.best_val_accuracy = val_accuracy\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= self.early_stopping_patience:\n",
    "                    print(\n",
    "                        \"Validation loss did not improve for %d epochs. Stopping training.\"\n",
    "                        % self.early_stopping_patience\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            self.test_model()\n",
    "\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "\n",
    "        avg_accuracy = []\n",
    "        avg_loss = []\n",
    "        pbar = tqdm(unit=\"batch\", file=sys.stdout, total=len(self.train_loader))\n",
    "        for batch_idx, data in enumerate(self.train_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.autocast(self.device.type):\n",
    "                outputs = self.model(inputs)\n",
    "                loss = torch.nn.functional.cross_entropy(outputs, labels, label_smoothing=0.2)\n",
    "\n",
    "            self.scaler.scale(loss).backward()      # scale loss trước khi backward\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            self.scaler.update()\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            batch_accuracy = (predictions == labels).sum().item() / labels.size(0)\n",
    "            avg_accuracy.append(batch_accuracy)\n",
    "            avg_loss.append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": np.mean(avg_loss), \"accuracy\": np.mean(avg_accuracy) * 100.0})\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "        return np.mean(avg_loss), np.mean(avg_accuracy) * 100.0\n",
    "    \n",
    "    def val_epoch(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        avg_loss = []\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "\n",
    "        pbar = tqdm(\n",
    "            unit=\"batch\", file=sys.stdout, total=len(self.val_loader)\n",
    "        )\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(self.val_loader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.autocast(self.device.type):\n",
    "                outputs = self.model(inputs)\n",
    "                loss = torch.nn.functional.cross_entropy(outputs, labels, label_smoothing=0.2)\n",
    "            \n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            avg_loss.append(loss.item())\n",
    "            predicted_labels.extend(predictions.tolist())\n",
    "            true_labels.extend(labels.tolist())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "        accuracy = (\n",
    "            torch.eq(torch.tensor(predicted_labels), torch.tensor(true_labels))\n",
    "            .float()\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        return np.mean(avg_loss), accuracy * 100.0\n",
    "    \n",
    "    def test_model(self):\n",
    "\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "\n",
    "        pbar = tqdm(unit=\"batch\", file=sys.stdout, total=len(self.test_loader))\n",
    "        for batch_idx, (inputs, labels) in enumerate(self.test_loader):\n",
    "            bs, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.autocast(self.device.type):\n",
    "                logits = self.model(inputs)\n",
    "            outputs_avg = logits.view(bs, ncrops, -1).mean(1)\n",
    "            predictions = torch.argmax(outputs_avg, dim=1)\n",
    "\n",
    "            predicted_labels.extend(predictions.tolist())\n",
    "            true_labels.extend(labels.tolist())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        accuracy = (\n",
    "            torch.eq(torch.tensor(predicted_labels), torch.tensor(true_labels))\n",
    "            .float()\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        print(\"Test Accuracy: %.4f %%\" % (accuracy * 100.0))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "        logger.info(f\"Model saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5589e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 14:25:52,480: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2025-10-14 14:25:52,486: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2025-10-14 14:25:52,487: INFO: common]: Directory created at: artifacts\n",
      "[2025-10-14 14:25:52,488: INFO: common]: Directory created at: artifacts\\training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namnh\\AppData\\Local\\Temp\\ipykernel_4792\\3924460229.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [04:10<00:00,  1.20batch/s, loss=1.8, accuracy=30.6] \n",
      "100%|██████████| 3589/3589 [01:38<00:00, 36.30batch/s]\n",
      "[2025-10-14 14:31:51,825: INFO: 3924460229]: Epoch [1/100], Train Loss: 1.8038, Train Accuracy: 30.63%, Val Loss: 1.7855, Val Accuracy: 34.94%\n",
      "[2025-10-14 14:31:52,042: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:40<00:00,  2.77batch/s]\n",
      "Test Accuracy: 32.3767 %\n",
      "100%|██████████| 300/300 [00:38<00:00,  7.77batch/s, loss=1.73, accuracy=37.6]\n",
      "100%|██████████| 3589/3589 [01:12<00:00, 49.60batch/s]\n",
      "[2025-10-14 14:34:23,866: INFO: 3924460229]: Epoch [2/100], Train Loss: 1.7291, Train Accuracy: 37.55%, Val Loss: 1.7118, Val Accuracy: 38.87%\n",
      "[2025-10-14 14:34:24,007: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.75batch/s]\n",
      "Test Accuracy: 36.7790 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.44batch/s, loss=1.69, accuracy=41.1]\n",
      "100%|██████████| 3589/3589 [01:08<00:00, 52.04batch/s]\n",
      "[2025-10-14 14:36:23,835: INFO: 3924460229]: Epoch [3/100], Train Loss: 1.6880, Train Accuracy: 41.07%, Val Loss: 1.6719, Val Accuracy: 42.85%\n",
      "[2025-10-14 14:36:23,965: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:11<00:00, 10.25batch/s]\n",
      "Test Accuracy: 46.6704 %\n",
      "100%|██████████| 300/300 [00:41<00:00,  7.29batch/s, loss=1.66, accuracy=42.6]\n",
      "100%|██████████| 3589/3589 [01:10<00:00, 51.27batch/s]\n",
      "[2025-10-14 14:38:26,143: INFO: 3924460229]: Epoch [4/100], Train Loss: 1.6566, Train Accuracy: 42.64%, Val Loss: 1.6293, Val Accuracy: 45.22%\n",
      "[2025-10-14 14:38:26,268: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.71batch/s]\n",
      "Test Accuracy: 51.6300 %\n",
      "100%|██████████| 300/300 [00:39<00:00,  7.69batch/s, loss=1.63, accuracy=44.2]\n",
      "100%|██████████| 3589/3589 [00:58<00:00, 61.63batch/s]\n",
      "[2025-10-14 14:40:14,087: INFO: 3924460229]: Epoch [5/100], Train Loss: 1.6339, Train Accuracy: 44.19%, Val Loss: 1.6222, Val Accuracy: 44.75%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.19batch/s]\n",
      "Test Accuracy: 49.2059 %\n",
      "100%|██████████| 300/300 [00:37<00:00,  7.94batch/s, loss=1.62, accuracy=45.2]\n",
      "100%|██████████| 3589/3589 [01:00<00:00, 59.55batch/s]\n",
      "[2025-10-14 14:42:02,248: INFO: 3924460229]: Epoch [6/100], Train Loss: 1.6210, Train Accuracy: 45.21%, Val Loss: 1.6051, Val Accuracy: 46.31%\n",
      "[2025-10-14 14:42:02,376: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.93batch/s]\n",
      "Test Accuracy: 51.9086 %\n",
      "100%|██████████| 300/300 [00:37<00:00,  7.91batch/s, loss=1.6, accuracy=46.3] \n",
      "100%|██████████| 3589/3589 [01:04<00:00, 55.81batch/s]\n",
      "[2025-10-14 14:43:54,978: INFO: 3924460229]: Epoch [7/100], Train Loss: 1.6023, Train Accuracy: 46.31%, Val Loss: 1.5948, Val Accuracy: 47.78%\n",
      "[2025-10-14 14:43:55,109: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.70batch/s]\n",
      "Test Accuracy: 53.1624 %\n",
      "100%|██████████| 300/300 [00:43<00:00,  6.87batch/s, loss=1.59, accuracy=47.4]\n",
      "100%|██████████| 3589/3589 [01:11<00:00, 50.00batch/s]\n",
      "[2025-10-14 14:46:01,149: INFO: 3924460229]: Epoch [8/100], Train Loss: 1.5939, Train Accuracy: 47.44%, Val Loss: 1.5786, Val Accuracy: 47.98%\n",
      "[2025-10-14 14:46:01,271: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:11<00:00, 10.22batch/s]\n",
      "Test Accuracy: 54.6392 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.58, accuracy=47.8]\n",
      "100%|██████████| 3589/3589 [01:04<00:00, 55.41batch/s]\n",
      "[2025-10-14 14:47:57,361: INFO: 3924460229]: Epoch [9/100], Train Loss: 1.5843, Train Accuracy: 47.83%, Val Loss: 1.5800, Val Accuracy: 48.23%\n",
      "[2025-10-14 14:47:57,476: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.85batch/s]\n",
      "Test Accuracy: 56.3110 %\n",
      "100%|██████████| 300/300 [00:38<00:00,  7.82batch/s, loss=1.58, accuracy=48.8]\n",
      "100%|██████████| 3589/3589 [00:59<00:00, 60.46batch/s]\n",
      "[2025-10-14 14:49:45,606: INFO: 3924460229]: Epoch [10/100], Train Loss: 1.5750, Train Accuracy: 48.80%, Val Loss: 1.5693, Val Accuracy: 48.51%\n",
      "[2025-10-14 14:49:45,732: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.19batch/s]\n",
      "Test Accuracy: 55.8094 %\n",
      "100%|██████████| 300/300 [00:37<00:00,  8.00batch/s, loss=1.57, accuracy=49.1]\n",
      "100%|██████████| 3589/3589 [01:04<00:00, 55.95batch/s]\n",
      "[2025-10-14 14:51:37,508: INFO: 3924460229]: Epoch [11/100], Train Loss: 1.5671, Train Accuracy: 49.09%, Val Loss: 1.5580, Val Accuracy: 49.87%\n",
      "[2025-10-14 14:51:37,628: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.24batch/s]\n",
      "Test Accuracy: 55.0850 %\n",
      "100%|██████████| 300/300 [00:37<00:00,  8.00batch/s, loss=1.56, accuracy=49.6]\n",
      "100%|██████████| 3589/3589 [01:02<00:00, 57.52batch/s]\n",
      "[2025-10-14 14:53:27,566: INFO: 3924460229]: Epoch [12/100], Train Loss: 1.5590, Train Accuracy: 49.63%, Val Loss: 1.5555, Val Accuracy: 49.71%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.10batch/s]\n",
      "Test Accuracy: 55.8373 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.44batch/s, loss=1.55, accuracy=50.3]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.19batch/s]\n",
      "[2025-10-14 14:55:24,279: INFO: 3924460229]: Epoch [13/100], Train Loss: 1.5504, Train Accuracy: 50.34%, Val Loss: 1.5451, Val Accuracy: 50.82%\n",
      "[2025-10-14 14:55:24,406: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.91batch/s]\n",
      "Test Accuracy: 56.7568 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.43batch/s, loss=1.54, accuracy=50.7]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.12batch/s]\n",
      "[2025-10-14 14:57:21,493: INFO: 3924460229]: Epoch [14/100], Train Loss: 1.5435, Train Accuracy: 50.69%, Val Loss: 1.5530, Val Accuracy: 50.40%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.06batch/s]\n",
      "Test Accuracy: 56.9518 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.43batch/s, loss=1.54, accuracy=51.1]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.25batch/s]\n",
      "[2025-10-14 14:59:19,503: INFO: 3924460229]: Epoch [15/100], Train Loss: 1.5388, Train Accuracy: 51.09%, Val Loss: 1.5594, Val Accuracy: 50.15%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.91batch/s]\n",
      "Test Accuracy: 56.6732 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.46batch/s, loss=1.53, accuracy=51.7]\n",
      "100%|██████████| 3589/3589 [01:04<00:00, 55.76batch/s]\n",
      "[2025-10-14 15:01:14,447: INFO: 3924460229]: Epoch [16/100], Train Loss: 1.5306, Train Accuracy: 51.67%, Val Loss: 1.5395, Val Accuracy: 50.96%\n",
      "[2025-10-14 15:01:14,563: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.00batch/s]\n",
      "Test Accuracy: 56.9239 %\n",
      "100%|██████████| 300/300 [00:37<00:00,  8.00batch/s, loss=1.53, accuracy=51.6]\n",
      "100%|██████████| 3589/3589 [01:02<00:00, 57.79batch/s]\n",
      "[2025-10-14 15:03:04,449: INFO: 3924460229]: Epoch [17/100], Train Loss: 1.5274, Train Accuracy: 51.60%, Val Loss: 1.5252, Val Accuracy: 52.33%\n",
      "[2025-10-14 15:03:04,588: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.03batch/s]\n",
      "Test Accuracy: 57.5369 %\n",
      "100%|██████████| 300/300 [00:37<00:00,  7.97batch/s, loss=1.52, accuracy=52.2]\n",
      "100%|██████████| 3589/3589 [01:00<00:00, 59.21batch/s]\n",
      "[2025-10-14 15:04:53,091: INFO: 3924460229]: Epoch [18/100], Train Loss: 1.5195, Train Accuracy: 52.23%, Val Loss: 1.5460, Val Accuracy: 50.91%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.21batch/s]\n",
      "Test Accuracy: 58.5678 %\n",
      "100%|██████████| 300/300 [00:38<00:00,  7.80batch/s, loss=1.52, accuracy=52.6]\n",
      "100%|██████████| 3589/3589 [01:05<00:00, 54.40batch/s]\n",
      "[2025-10-14 15:06:47,619: INFO: 3924460229]: Epoch [19/100], Train Loss: 1.5180, Train Accuracy: 52.64%, Val Loss: 1.5233, Val Accuracy: 53.00%\n",
      "[2025-10-14 15:06:47,760: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.95batch/s]\n",
      "Test Accuracy: 58.9022 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.42batch/s, loss=1.51, accuracy=52.6]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.95batch/s]\n",
      "[2025-10-14 15:08:45,053: INFO: 3924460229]: Epoch [20/100], Train Loss: 1.5126, Train Accuracy: 52.64%, Val Loss: 1.5285, Val Accuracy: 51.55%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.07batch/s]\n",
      "Test Accuracy: 58.9579 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.51, accuracy=53.1]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.99batch/s]\n",
      "[2025-10-14 15:10:42,039: INFO: 3924460229]: Epoch [21/100], Train Loss: 1.5065, Train Accuracy: 53.06%, Val Loss: 1.5311, Val Accuracy: 52.66%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.91batch/s]\n",
      "Test Accuracy: 58.8465 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.39batch/s, loss=1.51, accuracy=53.2]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.63batch/s]\n",
      "[2025-10-14 15:12:39,924: INFO: 3924460229]: Epoch [22/100], Train Loss: 1.5072, Train Accuracy: 53.22%, Val Loss: 1.5206, Val Accuracy: 52.66%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.98batch/s]\n",
      "Test Accuracy: 57.5926 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.5, accuracy=54]   \n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.35batch/s]\n",
      "[2025-10-14 15:14:36,527: INFO: 3924460229]: Epoch [23/100], Train Loss: 1.4998, Train Accuracy: 54.01%, Val Loss: 1.5127, Val Accuracy: 52.63%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.90batch/s]\n",
      "Test Accuracy: 59.4595 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.44batch/s, loss=1.5, accuracy=54.1] \n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.28batch/s]\n",
      "[2025-10-14 15:16:33,366: INFO: 3924460229]: Epoch [24/100], Train Loss: 1.4952, Train Accuracy: 54.10%, Val Loss: 1.5255, Val Accuracy: 52.35%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.94batch/s]\n",
      "Test Accuracy: 59.9053 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.43batch/s, loss=1.49, accuracy=54.4]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.02batch/s]\n",
      "[2025-10-14 15:18:30,540: INFO: 3924460229]: Epoch [25/100], Train Loss: 1.4917, Train Accuracy: 54.40%, Val Loss: 1.5198, Val Accuracy: 52.58%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.01batch/s]\n",
      "Test Accuracy: 58.2892 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.39batch/s, loss=1.49, accuracy=54.2]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.16batch/s]\n",
      "[2025-10-14 15:20:27,682: INFO: 3924460229]: Epoch [26/100], Train Loss: 1.4901, Train Accuracy: 54.24%, Val Loss: 1.4949, Val Accuracy: 53.86%\n",
      "[2025-10-14 15:20:27,816: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.99batch/s]\n",
      "Test Accuracy: 60.5461 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.46batch/s, loss=1.48, accuracy=54.9]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.65batch/s]\n",
      "[2025-10-14 15:22:25,241: INFO: 3924460229]: Epoch [27/100], Train Loss: 1.4808, Train Accuracy: 54.91%, Val Loss: 1.5189, Val Accuracy: 52.94%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.00batch/s]\n",
      "Test Accuracy: 60.0167 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.48, accuracy=55.5]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 52.84batch/s]\n",
      "[2025-10-14 15:24:23,721: INFO: 3924460229]: Epoch [28/100], Train Loss: 1.4802, Train Accuracy: 55.47%, Val Loss: 1.5110, Val Accuracy: 52.91%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.04batch/s]\n",
      "Test Accuracy: 60.2118 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.46batch/s, loss=1.48, accuracy=55.3]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.32batch/s]\n",
      "[2025-10-14 15:26:21,504: INFO: 3924460229]: Epoch [29/100], Train Loss: 1.4759, Train Accuracy: 55.30%, Val Loss: 1.5236, Val Accuracy: 52.47%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.87batch/s]\n",
      "Test Accuracy: 58.7907 %\n",
      "100%|██████████| 300/300 [00:41<00:00,  7.26batch/s, loss=1.47, accuracy=55.6]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.33batch/s]\n",
      "[2025-10-14 15:28:20,555: INFO: 3924460229]: Epoch [30/100], Train Loss: 1.4731, Train Accuracy: 55.59%, Val Loss: 1.5010, Val Accuracy: 53.75%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.89batch/s]\n",
      "Test Accuracy: 61.0198 %\n",
      "100%|██████████| 300/300 [00:41<00:00,  7.31batch/s, loss=1.47, accuracy=56]  \n",
      "100%|██████████| 3589/3589 [01:07<00:00, 52.92batch/s]\n",
      "[2025-10-14 15:30:19,806: INFO: 3924460229]: Epoch [31/100], Train Loss: 1.4653, Train Accuracy: 56.04%, Val Loss: 1.5144, Val Accuracy: 53.72%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.85batch/s]\n",
      "Test Accuracy: 61.1312 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.47, accuracy=56.2]\n",
      "100%|██████████| 3589/3589 [01:08<00:00, 52.72batch/s]\n",
      "[2025-10-14 15:32:18,591: INFO: 3924460229]: Epoch [32/100], Train Loss: 1.4656, Train Accuracy: 56.16%, Val Loss: 1.4827, Val Accuracy: 54.56%\n",
      "[2025-10-14 15:32:18,714: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.00batch/s]\n",
      "Test Accuracy: 61.0755 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.41batch/s, loss=1.46, accuracy=56.6]\n",
      "100%|██████████| 3589/3589 [01:35<00:00, 37.49batch/s]\n",
      "[2025-10-14 15:34:45,247: INFO: 3924460229]: Epoch [33/100], Train Loss: 1.4619, Train Accuracy: 56.58%, Val Loss: 1.5036, Val Accuracy: 53.30%\n",
      "100%|██████████| 113/113 [00:43<00:00,  2.61batch/s]\n",
      "Test Accuracy: 61.7442 %\n",
      "100%|██████████| 300/300 [05:13<00:00,  1.04s/batch, loss=1.46, accuracy=56.5]\n",
      "100%|██████████| 3589/3589 [01:15<00:00, 47.47batch/s]\n",
      "[2025-10-14 15:41:57,588: INFO: 3924460229]: Epoch [34/100], Train Loss: 1.4605, Train Accuracy: 56.53%, Val Loss: 1.4945, Val Accuracy: 55.34%\n",
      "[2025-10-14 15:41:57,730: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.02batch/s]\n",
      "Test Accuracy: 62.9981 %\n",
      "100%|██████████| 300/300 [00:39<00:00,  7.53batch/s, loss=1.45, accuracy=57.3]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.92batch/s]\n",
      "[2025-10-14 15:43:54,401: INFO: 3924460229]: Epoch [35/100], Train Loss: 1.4501, Train Accuracy: 57.27%, Val Loss: 1.4904, Val Accuracy: 54.44%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.03batch/s]\n",
      "Test Accuracy: 62.7194 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.43batch/s, loss=1.45, accuracy=57.4]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.54batch/s]\n",
      "[2025-10-14 15:45:52,073: INFO: 3924460229]: Epoch [36/100], Train Loss: 1.4491, Train Accuracy: 57.40%, Val Loss: 1.5013, Val Accuracy: 54.11%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.93batch/s]\n",
      "Test Accuracy: 60.0724 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.47batch/s, loss=1.45, accuracy=56.9]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.85batch/s]\n",
      "[2025-10-14 15:47:49,249: INFO: 3924460229]: Epoch [37/100], Train Loss: 1.4491, Train Accuracy: 56.91%, Val Loss: 1.4924, Val Accuracy: 54.56%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.00batch/s]\n",
      "Test Accuracy: 60.8805 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.45, accuracy=57.5]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.38batch/s]\n",
      "[2025-10-14 15:49:47,057: INFO: 3924460229]: Epoch [38/100], Train Loss: 1.4458, Train Accuracy: 57.51%, Val Loss: 1.4992, Val Accuracy: 54.72%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.05batch/s]\n",
      "Test Accuracy: 61.3541 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.48batch/s, loss=1.45, accuracy=57.7]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.57batch/s]\n",
      "[2025-10-14 15:51:44,429: INFO: 3924460229]: Epoch [39/100], Train Loss: 1.4450, Train Accuracy: 57.68%, Val Loss: 1.4904, Val Accuracy: 54.89%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.95batch/s]\n",
      "Test Accuracy: 61.8278 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.47batch/s, loss=1.44, accuracy=57.9]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.07batch/s]\n",
      "[2025-10-14 15:53:41,301: INFO: 3924460229]: Epoch [40/100], Train Loss: 1.4369, Train Accuracy: 57.92%, Val Loss: 1.4912, Val Accuracy: 55.22%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.98batch/s]\n",
      "Test Accuracy: 63.1095 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.49batch/s, loss=1.43, accuracy=58.2]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.18batch/s]\n",
      "[2025-10-14 15:55:37,901: INFO: 3924460229]: Epoch [41/100], Train Loss: 1.4342, Train Accuracy: 58.18%, Val Loss: 1.5000, Val Accuracy: 54.67%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.20batch/s]\n",
      "Test Accuracy: 62.2736 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.47batch/s, loss=1.43, accuracy=58.4]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.00batch/s]\n",
      "[2025-10-14 15:57:34,657: INFO: 3924460229]: Epoch [42/100], Train Loss: 1.4319, Train Accuracy: 58.37%, Val Loss: 1.4894, Val Accuracy: 54.67%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.04batch/s]\n",
      "Test Accuracy: 62.5522 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.48batch/s, loss=1.42, accuracy=59.3]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.13batch/s]\n",
      "[2025-10-14 15:59:31,321: INFO: 3924460229]: Epoch [43/100], Train Loss: 1.4199, Train Accuracy: 59.33%, Val Loss: 1.4885, Val Accuracy: 55.70%\n",
      "[2025-10-14 15:59:31,448: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.93batch/s]\n",
      "Test Accuracy: 62.2179 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.47batch/s, loss=1.43, accuracy=58.8]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.91batch/s]\n",
      "[2025-10-14 16:01:28,522: INFO: 3924460229]: Epoch [44/100], Train Loss: 1.4266, Train Accuracy: 58.85%, Val Loss: 1.5085, Val Accuracy: 53.89%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.99batch/s]\n",
      "Test Accuracy: 61.8278 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.49batch/s, loss=1.42, accuracy=59]  \n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.21batch/s]\n",
      "[2025-10-14 16:03:25,079: INFO: 3924460229]: Epoch [45/100], Train Loss: 1.4240, Train Accuracy: 58.98%, Val Loss: 1.4922, Val Accuracy: 54.05%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.06batch/s]\n",
      "Test Accuracy: 61.7721 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.48batch/s, loss=1.42, accuracy=59.5]\n",
      "100%|██████████| 3589/3589 [01:05<00:00, 54.90batch/s]\n",
      "[2025-10-14 16:05:20,776: INFO: 3924460229]: Epoch [46/100], Train Loss: 1.4191, Train Accuracy: 59.52%, Val Loss: 1.4863, Val Accuracy: 55.50%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.95batch/s]\n",
      "Test Accuracy: 62.0786 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.49batch/s, loss=1.41, accuracy=59.7]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.95batch/s]\n",
      "[2025-10-14 16:07:17,688: INFO: 3924460229]: Epoch [47/100], Train Loss: 1.4146, Train Accuracy: 59.73%, Val Loss: 1.5044, Val Accuracy: 54.69%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.97batch/s]\n",
      "Test Accuracy: 62.4965 %\n",
      "100%|██████████| 300/300 [00:39<00:00,  7.51batch/s, loss=1.41, accuracy=59.5]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.25batch/s]\n",
      "[2025-10-14 16:09:14,100: INFO: 3924460229]: Epoch [48/100], Train Loss: 1.4144, Train Accuracy: 59.49%, Val Loss: 1.4816, Val Accuracy: 55.31%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.98batch/s]\n",
      "Test Accuracy: 62.1622 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.46batch/s, loss=1.41, accuracy=59.7]\n",
      "100%|██████████| 3589/3589 [01:05<00:00, 54.49batch/s]\n",
      "[2025-10-14 16:11:10,492: INFO: 3924460229]: Epoch [49/100], Train Loss: 1.4098, Train Accuracy: 59.69%, Val Loss: 1.4922, Val Accuracy: 55.11%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.10batch/s]\n",
      "Test Accuracy: 62.7194 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.41, accuracy=59.7]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.42batch/s]\n",
      "[2025-10-14 16:13:08,150: INFO: 3924460229]: Epoch [50/100], Train Loss: 1.4101, Train Accuracy: 59.65%, Val Loss: 1.4814, Val Accuracy: 56.37%\n",
      "[2025-10-14 16:13:08,282: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.86batch/s]\n",
      "Test Accuracy: 61.8835 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.41, accuracy=60.2]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.07batch/s]\n",
      "[2025-10-14 16:15:05,341: INFO: 3924460229]: Epoch [51/100], Train Loss: 1.4062, Train Accuracy: 60.22%, Val Loss: 1.4991, Val Accuracy: 54.58%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.08batch/s]\n",
      "Test Accuracy: 62.8309 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.48batch/s, loss=1.4, accuracy=60.7] \n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.85batch/s]\n",
      "[2025-10-14 16:17:02,318: INFO: 3924460229]: Epoch [52/100], Train Loss: 1.3993, Train Accuracy: 60.68%, Val Loss: 1.4936, Val Accuracy: 55.48%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.07batch/s]\n",
      "Test Accuracy: 63.5832 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.48batch/s, loss=1.39, accuracy=60.7]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.24batch/s]\n",
      "[2025-10-14 16:18:58,831: INFO: 3924460229]: Epoch [53/100], Train Loss: 1.3949, Train Accuracy: 60.71%, Val Loss: 1.4904, Val Accuracy: 55.28%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.80batch/s]\n",
      "Test Accuracy: 60.9919 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.42batch/s, loss=1.4, accuracy=60.8] \n",
      "100%|██████████| 3589/3589 [01:05<00:00, 54.40batch/s]\n",
      "[2025-10-14 16:20:55,722: INFO: 3924460229]: Epoch [54/100], Train Loss: 1.3968, Train Accuracy: 60.82%, Val Loss: 1.4821, Val Accuracy: 55.75%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.10batch/s]\n",
      "Test Accuracy: 62.7473 %\n",
      "100%|██████████| 300/300 [00:39<00:00,  7.50batch/s, loss=1.39, accuracy=61.3]\n",
      "100%|██████████| 3589/3589 [01:01<00:00, 58.09batch/s]\n",
      "[2025-10-14 16:22:47,689: INFO: 3924460229]: Epoch [55/100], Train Loss: 1.3881, Train Accuracy: 61.28%, Val Loss: 1.5021, Val Accuracy: 54.92%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.12batch/s]\n",
      "Test Accuracy: 62.1343 %\n",
      "100%|██████████| 300/300 [00:37<00:00,  7.97batch/s, loss=1.39, accuracy=61.4]\n",
      "100%|██████████| 3589/3589 [01:00<00:00, 59.26batch/s]\n",
      "[2025-10-14 16:24:36,057: INFO: 3924460229]: Epoch [56/100], Train Loss: 1.3895, Train Accuracy: 61.41%, Val Loss: 1.4741, Val Accuracy: 55.45%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.25batch/s]\n",
      "Test Accuracy: 62.6916 %\n",
      "100%|██████████| 300/300 [00:37<00:00,  7.97batch/s, loss=1.38, accuracy=61.4]\n",
      "100%|██████████| 3589/3589 [01:02<00:00, 57.13batch/s]\n",
      "[2025-10-14 16:26:26,573: INFO: 3924460229]: Epoch [57/100], Train Loss: 1.3843, Train Accuracy: 61.41%, Val Loss: 1.5002, Val Accuracy: 54.92%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.97batch/s]\n",
      "Test Accuracy: 63.4717 %\n",
      "100%|██████████| 300/300 [00:41<00:00,  7.30batch/s, loss=1.38, accuracy=62]  \n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.48batch/s]\n",
      "[2025-10-14 16:28:25,063: INFO: 3924460229]: Epoch [58/100], Train Loss: 1.3762, Train Accuracy: 61.96%, Val Loss: 1.4888, Val Accuracy: 55.08%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.90batch/s]\n",
      "Test Accuracy: 63.4717 %\n",
      "100%|██████████| 300/300 [00:41<00:00,  7.30batch/s, loss=1.38, accuracy=61.9]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.17batch/s]\n",
      "[2025-10-14 16:30:22,805: INFO: 3924460229]: Epoch [59/100], Train Loss: 1.3784, Train Accuracy: 61.91%, Val Loss: 1.4879, Val Accuracy: 55.59%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.88batch/s]\n",
      "Test Accuracy: 62.6080 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.42batch/s, loss=1.37, accuracy=62.6]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.04batch/s]\n",
      "[2025-10-14 16:32:21,311: INFO: 3924460229]: Epoch [60/100], Train Loss: 1.3735, Train Accuracy: 62.61%, Val Loss: 1.4838, Val Accuracy: 56.03%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.97batch/s]\n",
      "Test Accuracy: 62.4687 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.47batch/s, loss=1.37, accuracy=62.7]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.07batch/s]\n",
      "[2025-10-14 16:34:18,139: INFO: 3924460229]: Epoch [61/100], Train Loss: 1.3704, Train Accuracy: 62.75%, Val Loss: 1.4747, Val Accuracy: 56.48%\n",
      "[2025-10-14 16:34:18,314: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.97batch/s]\n",
      "Test Accuracy: 63.0259 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.43batch/s, loss=1.37, accuracy=62.7]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.40batch/s]\n",
      "[2025-10-14 16:36:16,221: INFO: 3924460229]: Epoch [62/100], Train Loss: 1.3666, Train Accuracy: 62.71%, Val Loss: 1.4932, Val Accuracy: 56.00%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.97batch/s]\n",
      "Test Accuracy: 63.2767 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.39batch/s, loss=1.36, accuracy=63]  \n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.45batch/s]\n",
      "[2025-10-14 16:38:14,273: INFO: 3924460229]: Epoch [63/100], Train Loss: 1.3620, Train Accuracy: 63.04%, Val Loss: 1.4736, Val Accuracy: 57.34%\n",
      "[2025-10-14 16:38:14,390: INFO: 3924460229]: Model saved at artifacts\\training\\model.pth\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.79batch/s]\n",
      "Test Accuracy: 63.4717 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.37batch/s, loss=1.37, accuracy=63.1]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.62batch/s]\n",
      "[2025-10-14 16:40:12,539: INFO: 3924460229]: Epoch [64/100], Train Loss: 1.3684, Train Accuracy: 63.14%, Val Loss: 1.4859, Val Accuracy: 56.34%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.92batch/s]\n",
      "Test Accuracy: 63.2488 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.36, accuracy=63.1]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.03batch/s]\n",
      "[2025-10-14 16:42:10,862: INFO: 3924460229]: Epoch [65/100], Train Loss: 1.3605, Train Accuracy: 63.13%, Val Loss: 1.4856, Val Accuracy: 55.84%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.91batch/s]\n",
      "Test Accuracy: 64.0290 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.35, accuracy=63.7]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.41batch/s]\n",
      "[2025-10-14 16:44:08,677: INFO: 3924460229]: Epoch [66/100], Train Loss: 1.3535, Train Accuracy: 63.72%, Val Loss: 1.4904, Val Accuracy: 54.78%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.91batch/s]\n",
      "Test Accuracy: 63.4439 %\n",
      "100%|██████████| 300/300 [00:41<00:00,  7.28batch/s, loss=1.35, accuracy=63.6]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 53.94batch/s]\n",
      "[2025-10-14 16:46:06,784: INFO: 3924460229]: Epoch [67/100], Train Loss: 1.3539, Train Accuracy: 63.62%, Val Loss: 1.4682, Val Accuracy: 56.65%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.83batch/s]\n",
      "Test Accuracy: 64.5584 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.44batch/s, loss=1.35, accuracy=64.2]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.20batch/s]\n",
      "[2025-10-14 16:48:05,018: INFO: 3924460229]: Epoch [68/100], Train Loss: 1.3454, Train Accuracy: 64.16%, Val Loss: 1.4875, Val Accuracy: 56.92%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.92batch/s]\n",
      "Test Accuracy: 64.1404 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.45batch/s, loss=1.35, accuracy=63.8]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.15batch/s]\n",
      "[2025-10-14 16:50:01,917: INFO: 3924460229]: Epoch [69/100], Train Loss: 1.3493, Train Accuracy: 63.80%, Val Loss: 1.4838, Val Accuracy: 56.67%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.97batch/s]\n",
      "Test Accuracy: 64.1962 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.50batch/s, loss=1.34, accuracy=64.6]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.24batch/s]\n",
      "[2025-10-14 16:51:59,665: INFO: 3924460229]: Epoch [70/100], Train Loss: 1.3408, Train Accuracy: 64.63%, Val Loss: 1.4754, Val Accuracy: 56.51%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.91batch/s]\n",
      "Test Accuracy: 65.1156 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.38batch/s, loss=1.34, accuracy=65.1]\n",
      "100%|██████████| 3589/3589 [01:06<00:00, 54.36batch/s]\n",
      "[2025-10-14 16:53:56,738: INFO: 3924460229]: Epoch [71/100], Train Loss: 1.3388, Train Accuracy: 65.08%, Val Loss: 1.4766, Val Accuracy: 56.31%\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.72batch/s]\n",
      "Test Accuracy: 63.8897 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.39batch/s, loss=1.34, accuracy=64.9]\n",
      "100%|██████████| 3589/3589 [01:07<00:00, 53.25batch/s]\n",
      "[2025-10-14 16:55:55,274: INFO: 3924460229]: Epoch [72/100], Train Loss: 1.3357, Train Accuracy: 64.85%, Val Loss: 1.4804, Val Accuracy: 56.56%\n",
      "100%|██████████| 113/113 [00:10<00:00, 11.06batch/s]\n",
      "Test Accuracy: 64.4469 %\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.47batch/s, loss=1.34, accuracy=65]  \n",
      " 64%|██████▍   | 2304/3589 [00:42<00:23, 54.45batch/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     training = Training(config=training_config)\n\u001b[32m      5\u001b[39m     training.get_model()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mTraining completed successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mTraining.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     98\u001b[39m     train_loss, train_accuracy = \u001b[38;5;28mself\u001b[39m.train_epoch()\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     val_loss, val_accuracy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m val_accuracy > \u001b[38;5;28mself\u001b[39m.best_val_accuracy:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mTraining.val_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    161\u001b[39m labels = labels.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(\u001b[38;5;28mself\u001b[39m.device.type):\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     loss = torch.nn.functional.cross_entropy(outputs, labels, label_smoothing=\u001b[32m0.2\u001b[39m)\n\u001b[32m    167\u001b[39m predictions = torch.argmax(outputs, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\FacialExpressionRecognition\\models\\emonext.py:247\u001b[39m, in \u001b[36mEmoNeXt.forward\u001b[39m\u001b[34m(self, x, labels)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    246\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.stn(x)\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# _, weights = self.attention(x)\u001b[39;00m\n\u001b[32m    249\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\FacialExpressionRecognition\\models\\emonext.py:239\u001b[39m, in \u001b[36mEmoNeXt.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m4\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownsample_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m         x = \u001b[38;5;28mself\u001b[39m.stages[i](x)\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm(\n\u001b[32m    242\u001b[39m         x.mean([-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m])\n\u001b[32m    243\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\namnh\\miniconda3\\envs\\FER_New\\Lib\\site-packages\\FacialExpressionRecognition\\models\\emonext.py:90\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data_format == \u001b[33m\"\u001b[39m\u001b[33mchannels_first\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     89\u001b[39m     u = x.mean(\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     s = \u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m.mean(\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     91\u001b[39m     x = (x - u) / torch.sqrt(s + \u001b[38;5;28mself\u001b[39m.eps)\n\u001b[32m     92\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.weight[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m] * x + \u001b[38;5;28mself\u001b[39m.bias[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 2308/3589 [01:00<00:23, 54.45batch/s]"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_model()\n",
    "    training.train()\n",
    "    logger.info(\"Training completed successfully.\")\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488dc222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FER_New",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
