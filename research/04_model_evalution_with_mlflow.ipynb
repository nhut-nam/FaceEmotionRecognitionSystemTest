{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02ec54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7fcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7c8647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\ProjectAI\\\\FaceEmotionRecognitionSystem'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a42592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    path_of_model: Path\n",
    "    data_path: Path\n",
    "    mlflow_uri: str\n",
    "    params_image_size: list\n",
    "    params_batch_size: int\n",
    "    all_params: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94e5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FacialExpressionRecognition.constants import *\n",
    "from FacialExpressionRecognition.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07f4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        eval_config = EvaluationConfig(\n",
    "            path_of_model = self.config.training.trained_model_path,\n",
    "            data_path = self.config.training.dataset_path,\n",
    "            mlflow_uri = \"https://dagshub.com/nhut-nam/FaceEmotionRecognitionSystemTest.mlflow\",\n",
    "            params_image_size = self.params.IMAGE_SIZE,\n",
    "            params_batch_size = self.params.BATCH_SIZE,\n",
    "            all_params = self.params\n",
    "        )\n",
    "        return eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd19eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\namnh\\miniconda3\\envs\\FER\\lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\namnh\\miniconda3\\envs\\FER\\lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Open the following link in your browser to authorize the client:\n",
      "https://dagshub.com/login/oauth/authorize?state=62a9032b-06fd-4aba-b4da-f6422aa4d5ff&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=1263e033746c5d952f32e0594f60c301c4c5408dff2baf2f85a331c959db8277\n",
      "\n",
      "\n",
      "[2025-09-28 18:48:25,842: INFO: _client]: HTTP Request: POST https://dagshub.com/login/oauth/middleman \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 18:48:26,220: INFO: _client]: HTTP Request: POST https://dagshub.com/login/oauth/access_token \"HTTP/1.1 200 OK\"\n",
      "[2025-09-28 18:48:26,638: INFO: _client]: HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as nhut-nam\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as nhut-nam\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 18:48:26,645: INFO: helpers]: Accessing as nhut-nam\n",
      "[2025-09-28 18:48:27,131: INFO: _client]: HTTP Request: GET https://dagshub.com/api/v1/repos/nhut-nam/FaceEmotionRecognitionSystemTest \"HTTP/1.1 200 OK\"\n",
      "[2025-09-28 18:48:27,562: INFO: _client]: HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"nhut-nam/FaceEmotionRecognitionSystemTest\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"nhut-nam/FaceEmotionRecognitionSystemTest\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 18:48:27,566: INFO: helpers]: Initialized MLflow to track repo \"nhut-nam/FaceEmotionRecognitionSystemTest\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository nhut-nam/FaceEmotionRecognitionSystemTest initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository nhut-nam/FaceEmotionRecognitionSystemTest initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 18:48:27,568: INFO: helpers]: Repository nhut-nam/FaceEmotionRecognitionSystemTest initialized!\n"
     ]
    }
   ],
   "source": [
    "# import dagshub\n",
    "# dagshub.init(repo_owner='nhut-nam', repo_name='FaceEmotionRecognitionSystemTest', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99bd2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from urllib.parse import urlparse\n",
    "import mlflow.pytorch\n",
    "from FacialExpressionRecognition.models.resnet34 import get_resnet34_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Load model\n",
    "        self.model = self.load_model(model_path=self.config.path_of_model, params=self.config.all_params)\n",
    "\n",
    "        # Data transformations\n",
    "        data_transforms = transforms.Compose([\n",
    "            transforms.Resize((self.config.params_image_size[0], self.config.params_image_size[1])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # Load dataset\n",
    "        train_dataset = ImageFolder(root=self.config.data_path + \"/FER2013/train\", transform=data_transforms)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=self.config.params_batch_size, shuffle=True)\n",
    "\n",
    "        val_dataset = ImageFolder(root=self.config.data_path + \"/FER2013/val\", transform=data_transforms)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=self.config.params_batch_size, shuffle=False)\n",
    "\n",
    "        test_dataset = ImageFolder(root=self.config.data_path + \"/FER2013/test\", transform=data_transforms)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=self.config.params_batch_size, shuffle=False)\n",
    "\n",
    "        # Evaluate model\n",
    "        train_accuracy, train_loss = self.evaluate_score(train_dataloader)\n",
    "        val_accuracy, val_loss = self.evaluate_score(val_dataloader)\n",
    "        test_accuracy, test_loss = self.evaluate_score(test_dataloader)\n",
    "\n",
    "        print(f'Accuracy of the model on the training dataset: {train_accuracy:.2f}%')\n",
    "        print(f'Accuracy of the model on the validation dataset: {val_accuracy:.2f}%')\n",
    "        print(f'Accuracy of the model on the test dataset: {test_accuracy:.2f}%')\n",
    "\n",
    "        self.metrics = {\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"test_loss\": test_loss\n",
    "        }\n",
    "\n",
    "        self.save_score()\n",
    "\n",
    "    def evaluate_score(self, data_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        losses = []\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in data_loader:\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy, sum(losses) / len(losses)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(model_path, params):\n",
    "        model = get_resnet34_model(pretrained=False, num_classes=params['NUM_CLASSES'])\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            for key, value in self.metrics.items():\n",
    "                mlflow.log_metric(key, value)\n",
    "\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.pytorch.log_model(self.model, \"model\", registered_model_name=self.config.all_params['MODEL_NAME'])\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(self.model, \"model\")\n",
    "\n",
    "    def save_score(self):\n",
    "        save_json(path=Path(\"scores.json\"), data=self.metrics)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4776fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 18:51:14,565: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2025-09-28 18:51:14,566: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2025-09-28 18:51:14,567: INFO: common]: Directory created at: artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namnh\\miniconda3\\envs\\FER\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\namnh\\miniconda3\\envs\\FER\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the training dataset: 22.27%\n",
      "Accuracy of the model on the validation dataset: 23.82%\n",
      "Accuracy of the model on the test dataset: 22.54%\n",
      "[2025-09-28 18:52:46,289: INFO: common]: JSON file saved at: scores.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/28 18:52:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'resnet34'.\n",
      "2025/09/28 18:53:45 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: resnet34, version 1\n",
      "Created version '1' of model 'resnet34'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run fortunate-carp-296 at: https://dagshub.com/nhut-nam/FaceEmotionRecognitionSystemTest.mlflow/#/experiments/0/runs/edcc7675dcbb42af81f804983d63ba27\n",
      "🧪 View experiment at: https://dagshub.com/nhut-nam/FaceEmotionRecognitionSystemTest.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evaluation_config()\n",
    "    model_eval = ModelEvaluation(config=eval_config)\n",
    "    model_eval.evaluate()\n",
    "    model_eval.log_into_mlflow()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fee1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
